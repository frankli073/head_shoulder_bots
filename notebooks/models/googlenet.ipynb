{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYwFUUiFa7MG",
        "outputId": "c89b3368-ab11-4fb0-a8dc-36df1b68bd0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "## Import desired packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "import statsmodels.api as sm\n",
        "import gc\n",
        "\n",
        "from sklearn import __version__ as sklearn_version\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import datetime\n",
        "\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images and labels\n",
        "# We make sure it loads RGB things\n",
        "def load_images(folder_path, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".png\"):\n",
        "            image = Image.open(os.path.join(folder_path, filename)).convert('RGB').resize((224, 224))\n",
        "            images.append(np.array(image))\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "folder_0 = \"/content/drive/MyDrive/data/non_hs\"\n",
        "folder_1 = \"/content/drive/MyDrive/data/hs\"\n",
        "\n",
        "# Mark non_head_and_shoulder as 0 and head_and_shoulder as 1\n",
        "images_0, labels_0 = load_images(folder_0, 0)\n",
        "images_1, labels_1 = load_images(folder_1, 1)\n",
        "\n",
        "images = np.array(images_0 + images_1)\n",
        "labels = np.array(labels_0 + labels_1)\n",
        "\n",
        "# Split data into train and test sets, we also stratify them \n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "KHGeDbEdfNQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GoogLeNet"
      ],
      "metadata": {
        "id": "ZyF974bpfXYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "# Load the pretrained InceptionV3 model without the top layers\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Define custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "googlenet = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the pretrained model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile model\n",
        "googlenet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train GoogLeNet (InceptionV3) model\n",
        "googlenet.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test/255, y_test), epochs=10)\n",
        "test_loss, test_accuracy = googlenet.evaluate(X_test / 255, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXDE3FgLfAAQ",
        "outputId": "9c63f813-d6f9-4469-dadf-4d1f890770f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "33/33 [==============================] - 195s 6s/step - loss: 0.2427 - accuracy: 0.9194 - val_loss: 0.1845 - val_accuracy: 0.9583\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 188s 6s/step - loss: 0.1095 - accuracy: 0.9696 - val_loss: 0.1482 - val_accuracy: 0.9621\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 185s 6s/step - loss: 0.0951 - accuracy: 0.9725 - val_loss: 0.1331 - val_accuracy: 0.9621\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 185s 6s/step - loss: 0.0847 - accuracy: 0.9706 - val_loss: 0.1189 - val_accuracy: 0.9659\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 190s 6s/step - loss: 0.0801 - accuracy: 0.9734 - val_loss: 0.1201 - val_accuracy: 0.9659\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 187s 6s/step - loss: 0.0824 - accuracy: 0.9715 - val_loss: 0.1202 - val_accuracy: 0.9621\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 177s 5s/step - loss: 0.0696 - accuracy: 0.9763 - val_loss: 0.1122 - val_accuracy: 0.9659\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 192s 6s/step - loss: 0.0730 - accuracy: 0.9734 - val_loss: 0.1360 - val_accuracy: 0.9621\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 184s 6s/step - loss: 0.0736 - accuracy: 0.9772 - val_loss: 0.1054 - val_accuracy: 0.9659\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 186s 6s/step - loss: 0.0717 - accuracy: 0.9753 - val_loss: 0.1025 - val_accuracy: 0.9697\n",
            "9/9 - 34s - loss: 0.1025 - accuracy: 0.9697 - 34s/epoch - 4s/step\n",
            "Test accuracy: 96.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7CHsSB0afZVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}