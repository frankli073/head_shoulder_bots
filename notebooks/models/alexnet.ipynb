{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28jjM6JYa8L6",
        "outputId": "2dcf28a0-3e9a-4919-ecd1-30f6621e4122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "## Import desired packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "import statsmodels.api as sm\n",
        "import gc\n",
        "\n",
        "from sklearn import __version__ as sklearn_version\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import datetime\n",
        "\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import gc\n"
      ],
      "metadata": {
        "id": "u2P1c0TUee9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images and labels\n",
        "# We make sure it loads RGB things\n",
        "def load_images(folder_path, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".png\"):\n",
        "            image = Image.open(os.path.join(folder_path, filename)).convert('RGB').resize((224, 224))\n",
        "            images.append(np.array(image))\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "folder_0 = \"/content/drive/MyDrive/data/non_hs\"\n",
        "folder_1 = \"/content/drive/MyDrive/data/hs\"\n",
        "\n",
        "# Mark non_head_and_shoulder as 0 and head_and_shoulder as 1\n",
        "images_0, labels_0 = load_images(folder_0, 0)\n",
        "images_1, labels_1 = load_images(folder_1, 1)\n",
        "\n",
        "images = np.array(images_0 + images_1)\n",
        "labels = np.array(labels_0 + labels_1)\n",
        "\n",
        "# Split data into train and test sets, we also stratify them \n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "Qi-skvLuessG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alexnet"
      ],
      "metadata": {
        "id": "0-UT0dq6fHBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def create_alexnet(input_shape=(224, 224, 3), num_classes=1):\n",
        "    model = tf.keras.Sequential([\n",
        "        Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((3, 3), strides=(2, 2)),\n",
        "        Conv2D(256, (5, 5), activation='relu', padding='same'),\n",
        "        MaxPooling2D((3, 3), strides=(2, 2)),\n",
        "        Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((3, 3), strides=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile AlexNet model\n",
        "alexnet = create_alexnet()\n",
        "alexnet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train AlexNet model\n",
        "alexnet.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test/255, y_test), epochs=10)\n",
        "test_loss, test_accuracy = alexnet.evaluate(X_test / 255, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vcxxsqTexbn",
        "outputId": "98e7062d-0a29-44aa-f1da-5c2cee47b712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "33/33 [==============================] - 216s 6s/step - loss: 0.7558 - accuracy: 0.8956 - val_loss: 0.3499 - val_accuracy: 0.9091\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 213s 6s/step - loss: 0.3266 - accuracy: 0.9089 - val_loss: 0.3048 - val_accuracy: 0.9091\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 212s 6s/step - loss: 0.3080 - accuracy: 0.9089 - val_loss: 0.3075 - val_accuracy: 0.9091\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 215s 7s/step - loss: 0.3082 - accuracy: 0.9089 - val_loss: 0.3088 - val_accuracy: 0.9091\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 211s 6s/step - loss: 0.3114 - accuracy: 0.9089 - val_loss: 0.3273 - val_accuracy: 0.9091\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 210s 6s/step - loss: 0.3153 - accuracy: 0.9089 - val_loss: 0.3066 - val_accuracy: 0.9091\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 209s 6s/step - loss: 0.3075 - accuracy: 0.9089 - val_loss: 0.3053 - val_accuracy: 0.9091\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 201s 6s/step - loss: 0.3105 - accuracy: 0.9089 - val_loss: 0.3047 - val_accuracy: 0.9091\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 208s 6s/step - loss: 0.3089 - accuracy: 0.9089 - val_loss: 0.3284 - val_accuracy: 0.9091\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 209s 6s/step - loss: 0.3187 - accuracy: 0.9089 - val_loss: 0.3046 - val_accuracy: 0.9091\n",
            "9/9 - 13s - loss: 0.3046 - accuracy: 0.9091 - 13s/epoch - 1s/step\n",
            "Test accuracy: 90.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iiWdEnjdfINr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}