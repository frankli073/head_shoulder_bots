{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "BsOaB2-sW_14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1. Importing Packages and Methods](#1_importing_packages_and_methods)\n",
        "## [2. Importing Data and Labeling](#2_importing_data_and_labeling)\n",
        "## [3. Modeling](#3_modeling)\n",
        "- ### [3.1 Resnet-50](#3_1_resnet_50)\n",
        "- ### [3.2 Alexnet](#3_2_alexnet)\n",
        "- ### [3.3 GoogLeNet](#3_3_googlenet)\n"
      ],
      "metadata": {
        "id": "Mc-wB8mXTmex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importing Packages and Methods<a id='1_importing_packages_and_methods'></a>"
      ],
      "metadata": {
        "id": "ErMfz2a4UmUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "## Import desired packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "import statsmodels.api as sm\n",
        "import gc\n",
        "\n",
        "from sklearn import __version__ as sklearn_version\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import datetime\n",
        "\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import gc\n"
      ],
      "metadata": {
        "id": "f_FfiVJEUqDr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Importing Data and Labeling<a id='2_importing_data_and_labeling'></a>"
      ],
      "metadata": {
        "id": "r985NK5RUrNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images and labels\n",
        "# We make sure it loads RGB things\n",
        "def load_images(folder_path, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".png\"):\n",
        "            image = Image.open(os.path.join(folder_path, filename)).convert('RGB').resize((224, 224))\n",
        "            images.append(np.array(image))\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "folder_0 = \"/content/drive/MyDrive/data/non_hs\"\n",
        "folder_1 = \"/content/drive/MyDrive/data/hs\"\n",
        "\n",
        "# Mark non_head_and_shoulder as 0 and head_and_shoulder as 1\n",
        "images_0, labels_0 = load_images(folder_0, 0)\n",
        "images_1, labels_1 = load_images(folder_1, 1)\n",
        "\n",
        "images = np.array(images_0 + images_1)\n",
        "labels = np.array(labels_0 + labels_1)\n",
        "\n",
        "# Split data into train and test sets, we also stratify them \n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test/255, y_test), epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skPBKZWCUtKm",
        "outputId": "f052d634-a658-4230-9986-c1acdeb7b84f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "33/33 [==============================] - 146s 4s/step - loss: 0.4854 - accuracy: 0.8805 - val_loss: 0.3003 - val_accuracy: 0.9091\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 160s 5s/step - loss: 0.3070 - accuracy: 0.9089 - val_loss: 0.2998 - val_accuracy: 0.9091\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 137s 4s/step - loss: 0.3129 - accuracy: 0.9089 - val_loss: 0.3031 - val_accuracy: 0.9091\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 146s 4s/step - loss: 0.3148 - accuracy: 0.9089 - val_loss: 0.2896 - val_accuracy: 0.9091\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 138s 4s/step - loss: 0.3068 - accuracy: 0.9089 - val_loss: 0.3872 - val_accuracy: 0.9091\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 135s 4s/step - loss: 0.3046 - accuracy: 0.9089 - val_loss: 0.2788 - val_accuracy: 0.9091\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 143s 4s/step - loss: 0.2721 - accuracy: 0.9089 - val_loss: 0.2716 - val_accuracy: 0.9091\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 143s 4s/step - loss: 0.2597 - accuracy: 0.9089 - val_loss: 0.2706 - val_accuracy: 0.9091\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 137s 4s/step - loss: 0.2410 - accuracy: 0.9089 - val_loss: 0.2355 - val_accuracy: 0.9091\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 136s 4s/step - loss: 0.2160 - accuracy: 0.9260 - val_loss: 0.2561 - val_accuracy: 0.9091\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f05b3d599c0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modeling <a id='3_modeling'></a>"
      ],
      "metadata": {
        "id": "mppUM53SWzBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Resnet 50"
      ],
      "metadata": {
        "id": "mCPQighWWwXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "# ... (Load images and labels, split data, and data augmentation code remains the same)\n",
        "\n",
        "# Load the pretrained ResNet50 model without the top layers\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Define custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "resnet_50 = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the pretrained model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile model\n",
        "resnet_50.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "resnet_50.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test/255, y_test), epochs=10)\n",
        "test_loss, test_accuracy = resnet_50.evaluate(X_test / 255, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfUOFbNXKne",
        "outputId": "ac56facc-4b30-4be2-c7e1-d353ce5406d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "33/33 [==============================] - 312s 9s/step - loss: 0.3467 - accuracy: 0.8805 - val_loss: 0.3059 - val_accuracy: 0.9091\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 303s 9s/step - loss: 0.2990 - accuracy: 0.9089 - val_loss: 0.2981 - val_accuracy: 0.9091\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 303s 9s/step - loss: 0.2920 - accuracy: 0.9089 - val_loss: 0.2927 - val_accuracy: 0.9091\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 303s 9s/step - loss: 0.2873 - accuracy: 0.9089 - val_loss: 0.2926 - val_accuracy: 0.9091\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 301s 9s/step - loss: 0.2861 - accuracy: 0.9089 - val_loss: 0.2900 - val_accuracy: 0.9091\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 307s 9s/step - loss: 0.2744 - accuracy: 0.9089 - val_loss: 0.2730 - val_accuracy: 0.9091\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 306s 9s/step - loss: 0.2622 - accuracy: 0.9089 - val_loss: 0.2628 - val_accuracy: 0.9091\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 305s 9s/step - loss: 0.2544 - accuracy: 0.9089 - val_loss: 0.2548 - val_accuracy: 0.9091\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 303s 9s/step - loss: 0.2447 - accuracy: 0.9089 - val_loss: 0.2457 - val_accuracy: 0.9091\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 302s 9s/step - loss: 0.2352 - accuracy: 0.9089 - val_loss: 0.2359 - val_accuracy: 0.9091\n",
            "9/9 - 52s - loss: 0.2359 - accuracy: 0.9091 - 52s/epoch - 6s/step\n",
            "Test accuracy: 90.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Alexnet"
      ],
      "metadata": {
        "id": "Q9JBF7INYRz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def create_alexnet(input_shape=(224, 224, 3), num_classes=1):\n",
        "    model = tf.keras.Sequential([\n",
        "        Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((3, 3), strides=(2, 2)),\n",
        "        Conv2D(256, (5, 5), activation='relu', padding='same'),\n",
        "        MaxPooling2D((3, 3), strides=(2, 2)),\n",
        "        Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((3, 3), strides=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile AlexNet model\n",
        "alexnet = create_alexnet()\n",
        "alexnet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train AlexNet model\n",
        "alexnet.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test/255, y_test), epochs=10)\n",
        "test_loss, test_accuracy = alexnet.evaluate(X_test / 255, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "jOM27_KUYUMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 GoogLeNet "
      ],
      "metadata": {
        "id": "5uHe9OhCYc79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "# Load the pretrained InceptionV3 model without the top layers\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Define custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "googlenet = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the pretrained model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile model\n",
        "googlenet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train GoogLeNet (InceptionV3) model\n",
        "googlenet.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test/255, y_test), epochs=10)\n",
        "test_loss, test_accuracy = googlenet.evaluate(X_test / 255, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "T98NLnliYiZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PvHsmb1QYcV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "znoMika-YcHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FAiqmRitYboD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images and labels\n",
        "\n",
        "# We make sure it loads RGB things\n",
        "def load_images(folder_path, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".png\"):\n",
        "            image = Image.open(os.path.join(folder_path, filename)).convert('RGB').resize((224, 224))\n",
        "            images.append(np.array(image))\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "folder_0 = \"/content/drive/MyDrive/data/non_hs\"\n",
        "folder_1 = \"/content/drive/MyDrive/data/hs\"\n",
        "\n",
        "images_0, labels_0 = load_images(folder_0, 0)\n",
        "images_1, labels_1 = load_images(folder_1, 1)\n",
        "\n",
        "images = np.array(images_0 + images_1)\n",
        "labels = np.array(labels_0 + labels_1)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test/255, y_test), epochs=10)"
      ],
      "metadata": {
        "id": "eKqL-DLKVaX0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}